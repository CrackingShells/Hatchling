services:
  llmtests:
    image: llmtests
    build:
      context: .
      dockerfile: ./Dockerfile
    volumes:
      - ./:/app
      - ./__temp__:/app/__temp__
    environment:
      # When using network_mode: host on Linux, use localhost
      - OLLAMA_API_URL=${OLLAMA_API_URL:-http://localhost:11434/api}
      - DEFAULT_MODEL=${DEFAULT_MODEL:-mistral-small3.1}
      - MCP_SERVER_PATH=${MCP_SERVER_PATH:-mcp_server_arithmetics.py}
    network_mode: ${NETWORK_MODE}
    # Use Docker's built-in interactive mode
    tty: true
    stdin_open: true
